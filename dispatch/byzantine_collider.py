#!/usr/bin/env python3
"""
æ‹œå åº­å¯¹æ’å™¨ (Byzantine Collider) - AI å¢å¼ºç‰ˆ
å¤šè§’åº¦åˆ†æå·¥å…· - å°†ä¸»é¢˜ä»ä¸åŒç»´åº¦è¿›è¡Œ"ç¢°æ’"ï¼Œäº§ç”Ÿæ´è§

ç”¨æ³•:
    python3 byzantine_collider.py <ä¸»é¢˜> [è¾“å‡ºç›®å½•]
    python3 byzantine_collider.py <ä¸»é¢˜> --no-ai    # çº¯æ¡†æ¶æ¨¡å¼
"""

import sys
import os
import json
import asyncio
import aiohttp
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# é»˜è®¤è¾“å‡ºç›®å½•
DEFAULT_OUTPUT_DIR = Path.home() / "ai_os/projects/Research/outputs/byzantine-collider"

# åˆ†æç»´åº¦
PERSPECTIVES = [
    {
        "name": "æŠ€æœ¯æ¶æ„",
        "icon": "ğŸ—ï¸",
        "focus": "æŠ€æœ¯æ ˆã€æ¶æ„è®¾è®¡ã€æ€§èƒ½ã€æ‰©å±•æ€§ã€ä»£ç è´¨é‡",
    },
    {"name": "äº§å“ä½“éªŒ", "icon": "ğŸ¯", "focus": "ç”¨æˆ·ç—›ç‚¹ã€æ ¸å¿ƒä»·å€¼ã€UX/UIã€ç•™å­˜è½¬åŒ–"},
    {
        "name": "å·¥ç¨‹æ•ˆç‡",
        "icon": "âš¡",
        "focus": "å¼€å‘æµç¨‹ã€CI/CDã€æµ‹è¯•è¦†ç›–ç‡ã€éƒ¨ç½²è¿ç»´",
    },
    {
        "name": "å•†ä¸šæ¨¡å¼",
        "icon": "ğŸ’°",
        "focus": "æˆæœ¬ç»“æ„ã€å˜ç°æœºä¼šã€ç«äº‰å£å’ã€å¢é•¿å¼•æ“",
    },
    {"name": "é£é™©å®‰å…¨", "icon": "ğŸ›¡ï¸", "focus": "å®‰å…¨æ¼æ´ã€åˆè§„é£é™©ã€ä¾èµ–å¯é æ€§ã€ç¾å¤‡"},
    {
        "name": "æœªæ¥è¶‹åŠ¿",
        "icon": "ğŸ”®",
        "focus": "æŠ€æœ¯è¶‹åŠ¿ã€å¸‚åœºæœºä¼šã€æå‰å¸ƒå±€ã€é¢ è¦†é£é™©",
    },
]


class LLMClient:
    """ç»Ÿä¸€ LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒ MiniMax å’Œ OpenAI"""

    def __init__(self):
        self.provider = None
        self.api_key = os.environ.get("MINIMAX_API_KEY")
        self.base_url = os.environ.get(
            "MINIMAX_BASE_URL", "https://api.minimax.io/v1/text/chatcompletion_v2"
        )
        self.model = os.environ.get("MINIMAX_MODEL", "MiniMax-M2.5")

        if self.api_key:
            self.provider = "minimax"
            return

        # å…¶æ¬¡ä½¿ç”¨ OpenAI
        self.api_key = os.environ.get("OPENAI_API_KEY")
        self.base_url = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = os.environ.get("LLM_MODEL", "gpt-4o")

        if self.api_key:
            self.provider = "openai"

    async def chat(
        self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 2000
    ) -> str:
        if self.provider == "minimax":
            return await self._call_minimax(messages, temperature, max_tokens)
        elif self.provider == "openai":
            return await self._call_openai(messages, temperature, max_tokens)
        return "[æ— æœ‰æ•ˆ API é…ç½®]"

    async def _call_minimax(
        self, messages: List[Dict], temperature: float, max_tokens: int
    ) -> str:
        minimax_messages = [
            {"role": m.get("role", "user"), "content": m.get("content", "")}
            for m in messages
        ]

        payload = {
            "model": self.model,
            "messages": minimax_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60),
                ) as resp:
                    if resp.status != 200:
                        return f"[API é”™è¯¯: {resp.status}]"
                    data = await resp.json()
                    return (
                        data.get("choices", [{}])[0]
                        .get("message", {})
                        .get("content", "[æ— å†…å®¹]")
                    )
        except Exception as e:
            return f"[è°ƒç”¨å¤±è´¥: {e}]"

    async def _call_openai(
        self, messages: List[Dict], temperature: float, max_tokens: int
    ) -> str:
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60),
                ) as resp:
                    if resp.status != 200:
                        return f"[API é”™è¯¯: {resp.status}]"
                    data = await resp.json()
                    return (
                        data.get("choices", [{}])[0]
                        .get("message", {})
                        .get("content", "[æ— å†…å®¹]")
                    )
        except Exception as e:
            return f"[è°ƒç”¨å¤±è´¥: {e}]"


def extract_lines(text: str, max_count: int = 5) -> List[str]:
    lines = []
    for line in text.split("\n"):
        line = line.strip()
        if line and len(line) > 5:
            line = line.lstrip("0123456789.>-) ")
            if line:
                lines.append(line)
                if len(lines) >= max_count:
                    break
    return lines


async def generate_collision(topic: str, use_ai: bool = True) -> Dict[str, Any]:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    client = None
    if use_ai:
        client = LLMClient()
        if not client.api_key:
            print("âš ï¸ æœªæ‰¾åˆ° API Key")
            client = None

    model_name = client.model if client else None
    analyses, summary, next_steps = [], "", []

    if client:
        print(f"ğŸ¤– AI æ¨¡å¼ ({client.provider}/{model_name})")

        # åªç”¨ 3 ä¸ªæ ¸å¿ƒç»´åº¦
        core_perspectives = PERSPECTIVES[:3]

        # æ„å»º prompts
        prompts = []
        for p in core_perspectives:
            prompt = f"ä»{p['name']}è§’åº¦åˆ†æã€Œ{topic}ã€ã€‚èšç„¦: {p['focus']}ã€‚ç»™å‡º3ä¸ªæ´è§+2ä¸ªå»ºè®®ã€‚ä¸­æ–‡ï¼Œ150å­—å†…ã€‚"
            prompts.append(prompt)

        # å¹¶è¡Œè°ƒç”¨
        results = await asyncio.gather(
            *[
                client.chat([{"role": "user", "content": p}], max_tokens=600)
                for p in prompts
            ]
        )

        for i, text in enumerate(results):
            p = core_perspectives[i]
            analyses.append(
                {
                    "perspective": p["name"],
                    "icon": p["icon"],
                    "focus": p["focus"],
                    "insights": extract_lines(text),
                    "recommendations": extract_lines(text, 3),
                    "full_analysis": text,
                }
            )

        # æ€»ç»“
        summary = await client.chat(
            [{"role": "user", "content": f"ä¸€å¥è¯æ€»ç»“ã€Œ{topic}ã€çš„æ ¸å¿ƒé—®é¢˜ã€‚30å­—å†…ã€‚"}],
            max_tokens=80,
        )
        next_steps_text = await client.chat(
            [{"role": "user", "content": f"ã€Œ{topic}ã€çš„3ä¸ªæœ€é‡è¦è¡ŒåŠ¨ã€‚ä¸€è¡Œä¸€ä¸ªã€‚"}],
            max_tokens=80,
        )
        next_steps = [l.strip() for l in next_steps_text.split("\n") if l.strip()]
    else:
        print("ğŸ“ æ¡†æ¶æ¨¡å¼")
        for p in PERSPECTIVES[:3]:
            analyses.append(
                {
                    "perspective": p["name"],
                    "icon": p["icon"],
                    "focus": p["focus"],
                    "insights": [],
                    "recommendations": [],
                    "full_analysis": None,
                }
            )

    return {
        "topic": topic,
        "timestamp": timestamp,
        "use_ai": client is not None,
        "provider": client.provider if client else None,
        "model": model_name,
        "perspectives": analyses,
        "summary": summary,
        "next_steps": next_steps,
    }


def save_collision(topic: str, data: Dict, output_dir: Optional[Path] = None) -> Path:
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    safe_topic = "".join(c for c in topic if c.isalnum() or c in " -_").strip()[:50]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{safe_topic}_{timestamp}.md"
    filepath = output_dir / filename

    ai_badge = " ğŸ¤–" if data.get("use_ai") else ""
    model_info = (
        f" ({data.get('provider')}/{data.get('model')})" if data.get("provider") else ""
    )

    md = f"""# æ‹œå åº­å¯¹æ’{ai_badge}: {topic}
Generated: {datetime.now().isoformat()}{model_info}

---

## ç»¼åˆåˆ†æ

{data.get("summary", "(æ— )")}

---

"""
    for p in data["perspectives"]:
        md += f"\n### {p['icon']} {p['perspective']}\n\n**èšç„¦**: {p.get('focus', '')}\n\n"
        if p.get("insights"):
            md += "**æ´è§**:\n" + "\n".join(f"- {i}" for i in p["insights"]) + "\n"
        if p.get("recommendations"):
            md += (
                "\n**å»ºè®®**:\n"
                + "\n".join(f"- {r}" for r in p["recommendations"])
                + "\n"
            )
        md += "\n---\n"

    md += "\n## ä¸‹ä¸€æ­¥è¡ŒåŠ¨\n\n"
    for i, step in enumerate(data.get("next_steps", []), 1):
        md += f"{i}. {step}\n"

    filepath.write_text(md, encoding="utf-8")
    json_path = filepath.with_suffix(".json")
    json_path.write_text(
        json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    return filepath


async def main_async():
    if len(sys.argv) < 2:
        print(__doc__)
        topic = input("è¯·è¾“å…¥ä¸»é¢˜: ").strip()
        if not topic:
            sys.exit(1)
    else:
        topic = " ".join([a for a in sys.argv[1:] if a != "--no-ai"])

    use_ai = "--no-ai" not in sys.argv

    print(f"\nğŸ¯ ä¸»é¢˜: {topic}")
    print("=" * 50)

    collision = await generate_collision(topic, use_ai=use_ai)
    output_path = save_collision(topic, collision)

    print(f"\nâœ… å®Œæˆ! è¾“å‡º: {output_path}")

    if collision.get("summary"):
        print("\n" + "=" * 50)
        print("ğŸ“Š ç»¼åˆåˆ†æ:")
        print("-" * 50)
        print(collision["summary"][:500])


def main():
    asyncio.run(main_async())


if __name__ == "__main__":
    main()
