#!/usr/bin/env python3
"""
ç½‘ç»œè°ƒç ”æ¨¡å— (Network Research Module)
ä¸ºæ‹œå åº­å¯¹æ’æä¾›äº‹å®è¾“å…¥

åŠŸèƒ½ï¼š
- å¤šå¼•æ“æœç´¢
- æ¥æºåˆ†çº§
- ä¸‰è§’éªŒè¯
- ç¼“å­˜æœºåˆ¶
"""

import json
import hashlib
import time
from datetime import datetime, timedelta
from typing import Optional
from dataclasses import dataclass, asdict
from enum import Enum

# ============ æ•°æ®æ¨¡å‹ ============


class SourceTier(Enum):
    TIER_1 = "tier1"  # ä¸€æ‰‹æºï¼šå®˜æ–¹æ–‡æ¡£/è®ºæ–‡/æ–°é—»ç¨¿
    TIER_2 = "tier2"  # äºŒæ‰‹æºï¼šç¤¾è¯„/èšåˆå¹³å°
    TIER_3 = "tier3"  # ä¸‰æ‰‹æºï¼šè®ºå›/ç¤¾äº¤åª’ä½“


@dataclass
class SearchResult:
    """å•æ¡æœç´¢ç»“æœ"""

    title: str
    url: str
    snippet: str
    source: str
    tier: SourceTier
    timestamp: Optional[str] = None
    credibility: float = 0.5  # 0-1


@dataclass
class Fact:
    """æå–çš„äº‹å®"""

    content: str
    sources: list[SearchResult]
    confidence: float  # 0-1
    is_disputed: bool = False


@dataclass
class ResearchReport:
    """è°ƒç ”æŠ¥å‘Š"""

    query: str
    facts: list[Fact]
    disputed_facts: list[Fact]
    timestamp: str
    sources_used: int


# ============ æœç´¢å¼•æ“ ============


class SearchEngine:
    """æœç´¢å¼•æ“åŸºç±»"""

    def search(self, query: str) -> list[SearchResult]:
        raise NotImplementedError


class MockSearchEngine(SearchEngine):
    """æ¨¡æ‹Ÿæœç´¢ï¼ˆå®é™…ä½¿ç”¨æ›¿æ¢ä¸ºçœŸå® APIï¼‰"""

    # ç™½åå•åŸŸå
    WHITELIST_DOMAINS = {
        # å®˜æ–¹/ä¸€æ‰‹æº
        "arxiv.org": SourceTier.TIER_1,
        "github.com": SourceTier.TIER_1,
        "docs.python.org": SourceTier.TIER_1,
        "developer.mozilla.org": SourceTier.TIER_1,
        "wikipedia.org": SourceTier.TIER_1,
        # æƒå¨æ–°é—»
        "reuters.com": SourceTier.TIER_1,
        "bloomberg.com": SourceTier.TIER_1,
        "wsj.com": SourceTier.TIER_1,
        # ç§‘æŠ€åª’ä½“
        "techcrunch.com": SourceTier.TIER_2,
        "wired.com": SourceTier.TIER_2,
        "theverge.com": SourceTier.TIER_2,
    }

    def search(self, query: str) -> list[SearchResult]:
        """æ¨¡æ‹Ÿæœç´¢ç»“æœ"""
        # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œè°ƒç”¨çœŸå®æœç´¢ API
        # Google Custom Search API / Bing Search API / SerpAPI

        results = [
            SearchResult(
                title=f"å…³äº {query} çš„å®˜æ–¹æ–‡æ¡£",
                url=f"https://docs.example.com/{query}",
                snippet=f"è¿™æ˜¯ {query} çš„å®˜æ–¹æ–‡æ¡£è¯´æ˜...",
                source="docs.example.com",
                tier=SourceTier.TIER_1,
                timestamp=datetime.now().isoformat(),
                credibility=0.9,
            ),
            SearchResult(
                title=f"{query} - ç»´åŸºç™¾ç§‘",
                url=f"https://en.wikipedia.org/wiki/{query}",
                snippet=f"ç»´åŸºç™¾ç§‘ä¸Šå…³äº {query} çš„ä»‹ç»...",
                source="wikipedia.org",
                tier=SourceTier.TIER_1,
                timestamp=datetime.now().isoformat(),
                credibility=0.85,
            ),
            SearchResult(
                title=f"ç¤¾åŒºè®¨è®ºï¼š{query} çš„ä¼˜ç¼ºç‚¹",
                url=f"https://reddit.com/r/example/{query}",
                snippet=f"Reddit ä¸Šå…³äº {query} çš„è®¨è®º...",
                source="reddit.com",
                tier=SourceTier.TIER_3,
                timestamp=datetime.now().isoformat(),
                credibility=0.5,
            ),
        ]
        return results


class DuckDuckGoSearch(SearchEngine):
    """DuckDuckGo æœç´¢ï¼ˆå…è´¹ï¼Œæ— éœ€ API keyï¼‰"""

    def search(self, query: str) -> list[SearchResult]:
        try:
            import requests
            from bs4 import BeautifulSoup

            url = "https://html.duckduckgo.com/html/"
            data = {"q": query}

            response = requests.post(url, data=data, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")

            results = []
            for result in soup.select(".result"):
                title_elem = result.select_one(".result__title")
                link_elem = result.select_one(".result__url")
                snippet_elem = result.select_one(".result__snippet")

                if title_elem and link_elem:
                    domain = (
                        link_elem.text.strip().split("/")[0] if link_elem.text else ""
                    )

                    results.append(
                        SearchResult(
                            title=title_elem.text.strip(),
                            url=link_elem.text.strip(),
                            snippet=snippet_elem.text.strip() if snippet_elem else "",
                            source=domain,
                            tier=self._classify_tier(domain),
                            timestamp=datetime.now().isoformat(),
                            credibility=0.7
                            if domain in self.WHITELIST_DOMAINS
                            else 0.5,
                        )
                    )

            return results[:10]  # é™åˆ¶ç»“æœæ•°

        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []

    def _classify_tier(self, domain: str) -> SourceTier:
        if domain in self.WHITELIST_DOMAINS:
            return self.WHITELIST_DOMAINS[domain]
        return SourceTier.TIER_3


# ============ è°ƒç ”æ ¸å¿ƒ ============


class NetworkResearcher:
    """ç½‘ç»œè°ƒç ”å™¨"""

    def __init__(self, cache_ttl: int = 86400):  # é»˜è®¤ 24 å°æ—¶
        self.engine = MockSearchEngine()  # å¯æ›¿æ¢ä¸ºçœŸå®å¼•æ“
        self.cache = {}  # ç®€å•å†…å­˜ç¼“å­˜
        self.cache_ttl = cache_ttl

    def research(self, query: str, enable_cache: bool = True) -> ResearchReport:
        """æ‰§è¡Œè°ƒç ”"""

        # æ£€æŸ¥ç¼“å­˜
        if enable_cache:
            cached = self._get_cached(query)
            if cached:
                print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {query}")
                return cached

        print(f"ğŸ” å¼€å§‹è°ƒç ”: {query}")

        # å¤šå¼•æ“æœç´¢
        all_results = []

        # ä¸»æœç´¢
        results = self.engine.search(query)
        all_results.extend(results)

        # è¡¥å……æœç´¢ï¼ˆä¸åŒè¡¨è¿°ï¼‰
        alt_queries = self._generate_alt_queries(query)
        for alt_q in alt_queries[:2]:
            alt_results = self.engine.search(alt_q)
            all_results.extend(alt_results)

        # å»é‡
        all_results = self._deduplicate(all_results)

        # æå–äº‹å®
        facts, disputed = self._extract_facts(all_results)

        # æ„å»ºæŠ¥å‘Š
        report = ResearchReport(
            query=query,
            facts=facts,
            disputed_facts=disputed,
            timestamp=datetime.now().isoformat(),
            sources_used=len(all_results),
        )

        # ç¼“å­˜
        if enable_cache:
            self._set_cached(query, report)

        return report

    def _generate_alt_queries(self, query: str) -> list[str]:
        """ç”Ÿæˆæ›¿ä»£æŸ¥è¯¢"""
        return [
            f"{query} ä¼˜ç¼ºç‚¹",
            f"{query} æ¡ˆä¾‹",
            f"{query} å¸‚åœºåˆ†æ",
        ]

    def _deduplicate(self, results: list[SearchResult]) -> list[SearchResult]:
        """å»é‡"""
        seen = set()
        unique = []
        for r in results:
            if r.url not in seen:
                seen.add(r.url)
                unique.append(r)
        return unique

    def _extract_facts(
        self, results: list[SearchResult]
    ) -> tuple[list[Fact], list[Fact]]:
        """ä»æœç´¢ç»“æœæå–äº‹å®"""

        # ç®€åŒ–ç‰ˆï¼šæŒ‰æ¥æºåˆ†ç»„
        tier1_results = [r for r in results if r.tier == SourceTier.TIER_1]

        facts = []
        disputed = []

        if tier1_results:
            # ç®€å•å¤„ç†ï¼šæ¯æ¡ TIER_1 ç»“æœç®—ä¸€ä¸ªäº‹å®
            for r in tier1_results:
                fact = Fact(
                    content=r.snippet,
                    sources=[r],
                    confidence=r.credibility,
                    is_disputed=False,
                )
                facts.append(fact)

        return facts, disputed

    def _get_cached(self, query: str) -> Optional[ResearchReport]:
        """è·å–ç¼“å­˜"""
        key = self._cache_key(query)
        if key in self.cache:
            cached = self.cache[key]
            age = time.time() - cached["time"]
            if age < self.cache_ttl:
                return cached["report"]
        return None

    def _set_cached(self, query: str, report: ResearchReport):
        """è®¾ç½®ç¼“å­˜"""
        key = self._cache_key(query)
        self.cache[key] = {"report": report, "time": time.time()}

    def _cache_key(self, query: str) -> str:
        return hashlib.md5(query.encode()).hexdigest()


# ============ å·¥å…·å‡½æ•° ============


def format_report(report: ResearchReport) -> str:
    """æ ¼å¼åŒ–è°ƒç ”æŠ¥å‘Š"""

    output = []
    output.append(f"# è°ƒç ”æŠ¥å‘Š: {report.query}")
    output.append(f"\nâ° æ—¶é—´: {report.timestamp}")
    output.append(f"ğŸ“Š æ¥æºæ•°: {report.sources_used}")
    output.append(f"âœ… ç¡®è®¤äº‹å®: {len(report.facts)}")
    output.append(f"â“ äº‰è®®äº‹å®: {len(report.disputed_facts)}")
    output.append("\n" + "=" * 50)

    if report.facts:
        output.append("\n## âœ… ç¡®è®¤äº‹å®\n")
        for i, fact in enumerate(report.facts, 1):
            output.append(f"### {i}. {fact.content[:100]}...")
            output.append(f"   ç½®ä¿¡åº¦: {fact.confidence:.0%}")
            for src in fact.sources[:2]:
                output.append(f"   - [{src.title}]({src.url})")
            output.append("")

    if report.disputed_facts:
        output.append("\n## â“ äº‰è®®äº‹å®\n")
        for i, fact in enumerate(report.disputed_facts, 1):
            output.append(f"### {i}. {fact.content[:100]}...")
            output.append("")

    return "\n".join(output)


# ============ ä¸»å…¥å£ ============

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ç½‘ç»œè°ƒç ”æ¨¡å—")
    parser.add_argument("--query", "-q", required=True, help="è°ƒç ”ä¸»é¢˜")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶")

    args = parser.parse_args()

    researcher = NetworkResearcher()
    report = researcher.research(args.query)

    # è¾“å‡º
    print(format_report(report))

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(asdict(report), f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {args.output}")
